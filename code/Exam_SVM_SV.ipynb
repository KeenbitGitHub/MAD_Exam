{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 2)\n",
      "(800,)\n",
      "(200, 2)\n",
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn import svm, model_selection\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# this one is only needed if you want to use dictionary in getOptimalSVM\n",
    "import operator\n",
    "\n",
    "# use seaborn plotting defaults\n",
    "import seaborn as sns \n",
    "sns.set()\n",
    "data = np.loadtxt('./dataSVM.txt')\n",
    "\n",
    "features = data[0:int(len(data) * 0.8), :-1]\n",
    "labels = data[0:int(len(data) * 0.8), 2]\n",
    "testingFeatures = data[int(len(data) * 0.8):, :-1]\n",
    "testingLabels = data[int(len(data) * 0.8):, 2]\n",
    "\n",
    "print(features.shape)\n",
    "print(labels.shape)\n",
    "print(testingFeatures.shape)\n",
    "print(testingLabels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(clf, X, Y, cmap = 'Paired_r'):\n",
    "    \"\"\"\n",
    "    This function visualizes the input data X, colored according to labels Y, \n",
    "    superimposed over the decision boundary of the classifier clf\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    h = 0.02\n",
    "    x_min, x_max = X[:,0].min() - 10*h, X[:,0].max() + 10*h\n",
    "    y_min, y_max = X[:,1].min() - 10*h, X[:,1].max() + 10*h\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.contourf(xx, yy, Z, cmap = cmap, alpha = 0.25)\n",
    "    plt.contour(xx, yy, Z, colors = 'k', linewidths = 0.7)\n",
    "    plt.scatter(X[:,0], X[:,1], c = Y, cmap = cmap, edgecolors = 'k');\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: it is only allowed to use imports specified at the top of the document, you should not write any addional import\n",
    "\n",
    "class CrossValidation_SVM:\n",
    "    \n",
    "    def __init__(self, n_folds = 5, parameterCList = [0.001, 0.01, 0.1, 1, 10, 100]):\n",
    "        \"\"\"\n",
    "        Initializes the cross-validation class.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        n_folds : the number of folds we expect to use for cross-validation\n",
    "        \"\"\"\n",
    "        \n",
    "        self.n_folds = n_folds\n",
    "        self.parameterCList = parameterCList\n",
    "    \n",
    "    def __generateNextSplit(self, features, labels, kfold):\n",
    "        \"\"\"\n",
    "        Use this function to generate the next split of the input features and labels into \n",
    "        training and validation parts\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        features : Array of shape NxM\n",
    "        labels   : Array of length N\n",
    "        kfold    : iterator\n",
    "        \n",
    "        N is the number of samples, M is the number of features\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        trainingFeatures   : subportion of the features selected for training using the current split\n",
    "        trainingLabels     : subportion of the labels selected for training using the current split\n",
    "        validationFeatures : subportion of the features selected for validation using the current split\n",
    "        validationLabels   : subportion of the labels selected for validation using the current split\n",
    "        \"\"\" \n",
    "        \n",
    "        # TODO: ADD YOUR CODE HERE\n",
    "        \n",
    "        return trainingFeatures, trainingLabels, validationFeatures, validationLabels\n",
    "    \n",
    "    def __computeErrorForSpecificC(self, features, labels, C):\n",
    "        \"\"\"\n",
    "        Iterate throught all splits and compute the SVM performance using the specified C.\n",
    "        Use a linear kernel for the SVM.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        features : Array of shape NxM\n",
    "        labels   : Array of length N\n",
    "        C : svm regularization parameter\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        meanAccuracy : mean validation accuracy computer over all splits\n",
    "        \"\"\"         \n",
    "        \n",
    "        accuracyArray = []\n",
    "        \n",
    "        # this function should use __generateNextSplit for iterating over splits\n",
    "        # The iterator over folds. If you don't want to use it, feel free to implement your own iterator\n",
    "        kfold = KFold(self.n_folds, shuffle = False)\n",
    "        \n",
    "        # TODO: ADD YOUR CODE HERE\n",
    "        \n",
    "        return np.mean(accuracyArray)\n",
    "    \n",
    "    def getOptimalSVM(self, features, labels):\n",
    "        \"\"\"\n",
    "        Iterate through all C from self.parameterCList to find the optimal one, and generate the corresponding SVM classifier\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        features : Array of shape NxM\n",
    "        labels   : Array of length N\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        clf : the optimal SVM classfier trained on all input features and labels using optimal C\n",
    "        accuracyList : the list of mean accuracies for each C from self.parameterCList.\n",
    "                       accuracyList should be of the length len(self.parameterCList) \n",
    "        \"\"\"\n",
    "        \n",
    "        accuracyList = []# if you want and know how, you can use dictionary here: accuracyDict = {}\n",
    "        \n",
    "        # TODO: ADD YOUR CODE HERE\n",
    "        #   1) go through all C in self.parameterCList using __computeErrorForSpecificC\n",
    "        #   2) memorize mean accuracies for different C into accuracyList\n",
    "        #   3) find the optimal C with the highest mean accuracy\n",
    "        #   4) train clf on all input features and label using the optimal C\n",
    "        \n",
    "        \n",
    "        return clf, accuracyList\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the code to test your cross-validation implementation.\n",
    "tester = CrossValidation_SVM(n_folds = 5, parameterCList = [0.001, 0.01, 0.1, 1, 10, 100])\n",
    "clf, accuracyList = tester.getOptimalSVM(features, labels)\n",
    "print(accuracyList)\n",
    "plot_decision_boundary(clf, testingFeatures, testingLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
